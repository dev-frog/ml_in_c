# ML

```text
However, it's common for each new version of the GPT series to have a larger number of parameters, which generally contributes to improved performance in terms of language understanding and generation. GPT-3, for instance, has 175 billion parameters, and if GPT-4 indeed has 1 trillion (1,000,000,000,000) parameters, it would represent a significant advancement in model size and complexity. Keep in mind that model parameters alone don't solely determine the model's capabilities; architecture and training data also play essential roles.

```

```c
  float y_hat = x * w;
  float loss = y_hat - y;
  float grad = loss * x;
  float lr = 0.01f;
  w -= lr * grad;
```

## Model

```math
y = x * w
```
